{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "from config import *\n",
    "from utils import *\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import articulate as art\n",
    "from articulate.utils.rbdl import *\n",
    "from net import PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIP(\n",
       "  (rnn1): RNNWithInit(\n",
       "    (rnn): LSTM(256, 256, num_layers=2, dropout=0.4)\n",
       "    (linear1): Linear(in_features=72, out_features=256, bias=True)\n",
       "    (linear2): Linear(in_features=256, out_features=15, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "    (init_net): Sequential(\n",
       "      (0): Linear(in_features=15, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (rnn2): RNN(\n",
       "    (rnn): LSTM(256, 256, num_layers=2, dropout=0.4)\n",
       "    (linear1): Linear(in_features=87, out_features=256, bias=True)\n",
       "    (linear2): Linear(in_features=256, out_features=69, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (rnn3): RNN(\n",
       "    (rnn): LSTM(256, 256, num_layers=2, dropout=0.4)\n",
       "    (linear1): Linear(in_features=141, out_features=256, bias=True)\n",
       "    (linear2): Linear(in_features=256, out_features=90, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (rnn4): RNNWithInit(\n",
       "    (rnn): LSTM(256, 256, num_layers=2, dropout=0.4)\n",
       "    (linear1): Linear(in_features=141, out_features=256, bias=True)\n",
       "    (linear2): Linear(in_features=256, out_features=72, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "    (init_net): Sequential(\n",
       "      (0): Linear(in_features=72, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (rnn5): RNN(\n",
       "    (rnn): LSTM(64, 64, num_layers=2, dropout=0.4)\n",
       "    (linear1): Linear(in_features=141, out_features=64, bias=True)\n",
       "    (linear2): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = PIP()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/dataset_work/TotalCapture', 'TotalCapture')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = paths.totalcapture_dir\n",
    "\n",
    "data_name = os.path.basename(data_dir)\n",
    "\n",
    "data_dir, data_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_all, rot_all, pose_all, tran_all = torch.load(os.path.join(data_dir, 'test.pt')).values()\n",
    "init_poses = [art.math.axis_angle_to_rotation_matrix(_[0]) for _ in pose_all]\n",
    "\n",
    "sequence_ids = list(range(len(acc_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 41, 41, 45, 41)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acc_all), len(rot_all), len(pose_all), len(tran_all), len(init_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4113, 6, 3]) torch.Size([4113, 6, 3, 3])\n",
      "torch.Size([4113, 24, 3]) torch.Size([4113, 3])\n",
      "torch.Size([24, 3, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    for i in tqdm.tqdm(sequence_ids):\n",
    "        acc_t, rot_t = acc_all[i], rot_all[i]\n",
    "        pose_t, tran_t = pose_all[i], tran_all[i]\n",
    "        init_pose_t = init_poses[i]\n",
    "        print(acc_t.shape, rot_t.shape)\n",
    "        print(pose_t.shape, tran_t.shape)\n",
    "        print(init_pose_t.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## config.py setting\n",
    "# joint_set\n",
    "# vel_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x, lj_init, jvel_init = list(zip(*[x]))\n",
    "\n",
    "# x[0].shape, lj_init[0].shape, jvel_init[0].shape\n",
    "# x[0].dtype, lj_init[0].dtype, jvel_init[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "        glb_acc, glb_rot, init_pose = acc_t.to(device), rot_t.to(device), init_pose_t.to(device)\n",
    "        \n",
    "        # init pose\n",
    "        init_pose = init_pose.view(1, 24, 3, 3)\n",
    "        init_pose[0, 0] = torch.eye(3)\n",
    "        # global pose\n",
    "        pose_global, joint_global = net.forward_kinematics(init_pose) # calc_mesh == False\n",
    "        # leaf(1 order) joint\n",
    "        lj_init = joint_global[0, joint_set.leaf].view(-1)\n",
    "        # init velocity\n",
    "        jvel_init = torch.zeros(24 * 3)\n",
    "        # init all joint pose x\n",
    "        # x = (normalize_and_concat(glb_acc, glb_rot), lj_init, jvel_init) # x = (frames, 72)\n",
    "        x = (normalize_and_concat(glb_acc, glb_rot).to(device), \n",
    "             lj_init.to(device), \n",
    "             jvel_init.to(device)) # x = (frames, 72)\n",
    "        # Neural Kinematic Estimator\n",
    "        leaf_joint, full_joint, global_6d_pose, joint_velocity, contact = [_[0] for _ in net.forward([x])]\n",
    "        pose = net._reduced_glb_6d_to_full_local_mat(glb_rot.view(-1, 6, 3, 3)[:, -1], global_6d_pose)\n",
    "        joint_velocity = joint_velocity.view(-1, 24, 3).bmm(glb_rot[:, -1].transpose(1, 2)) * vel_scale\n",
    "        \n",
    "        # compute loss\n",
    "        \n",
    "        # detach grad\n",
    "        pose = pose.cpu().detach()\n",
    "        joint_velocity = joint_velocity.cpu().detach()\n",
    "        contact = contact.cpu().detach()\n",
    "        glb_acc = glb_acc.cpu().detach()\n",
    "        \n",
    "        # Physics-aware Motion Optimizer\n",
    "        pose_opt, tran_opt = [], []\n",
    "        for p, v, c, a in zip(pose, joint_velocity, contact, glb_acc):\n",
    "            p, t = net.dynamics_optimizer.optimize_frame(p, v, c, a)\n",
    "            pose_opt.append(p)\n",
    "            tran_opt.append(t)\n",
    "        pose_opt, tran_opt = torch.stack(pose_opt), torch.stack(tran_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip 200 frames\n",
    "# batch 256\n",
    "# Adam optimizer\n",
    "# fine-tune PL (with IPL), PA, and RA on the train split of the DIP-IMU \n",
    "# do not train VA and CF on DIP-IMU (does not contain global movements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_function():\n",
    "    L_PL = torch.nn.functional.mse_loss(leaf_joint, ) # sec.3.1.1\n",
    "    L_PA = torch.nn.functional.mse_loss(full_joint, ) # sec.3.1.1\n",
    "    L_RA = torch.nn.functional.mse_loss(global_6d_pose, ) # sec.3.1.1\n",
    "    \n",
    "    L_CF = torch.nn.functional.binary_cross_entropy(contact, ) # sec.3.1.1\n",
    "    \n",
    "    L_VA = torch.nn.functional.(joint_velocity, ) # [73]:TransPose eq.(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_velocity_GT = ()() / delta_t # sec.A eq.(14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
